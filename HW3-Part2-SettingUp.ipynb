{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 2: Setting up `Cart-Pole` Agent.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`Name`** : **Pavaris Asawakijtananont**\n",
    "\n",
    "- **`Number`** : **65340500037**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Configuration**\n",
    "#### **Reward Function**\n",
    "- Including with 5 term of reward the duration of episode can approximate equal reward value\n",
    "\n",
    "```python\n",
    "class RewardsCfg:\n",
    "    \"\"\"Reward terms for the MDP.\"\"\"\n",
    "\n",
    "    # (1) Constant running reward\n",
    "    alive = RewTerm(func=mdp.is_alive, weight=1.0)\n",
    "    # (2) Failure penalty\n",
    "    terminating = RewTerm(func=mdp.is_terminated, weight=-2.0)\n",
    "    # (3) Primary task: keep pole upright\n",
    "    pole_pos = RewTerm(\n",
    "        func=mdp.joint_pos_target_l2,\n",
    "        weight=-1.0,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"]), \"target\": 0.0},\n",
    "    )\n",
    "    # (4) Shaping tasks: lower cart velocity\n",
    "    cart_vel = RewTerm(\n",
    "        func=mdp.joint_vel_l1,\n",
    "        weight=-0.01,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"slider_to_cart\"])},\n",
    "    )\n",
    "    # (5) Shaping tasks: lower pole angular velocity\n",
    "    pole_vel = RewTerm(\n",
    "        func=mdp.joint_vel_l1,\n",
    "        weight=-0.005,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"])},\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Base Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **q**\n",
    "- calling the action value with using linear approximator to use with Linear Q Learning \n",
    "\n",
    "```python\n",
    "    def q(self, obs, a=None):\n",
    "        \"\"\"Returns the linearly-estimated Q-value for a given state and action.\"\"\"\n",
    "        obs_val = obs['policy'][0].detach().cpu().numpy()\n",
    "        if a==None:\n",
    "            # Get q values from all action in state\n",
    "            return np.dot(obs_val, self.w)\n",
    "        else:\n",
    "            # Get q values given action & state\n",
    "            return np.dot(obs_val, self.w[:, a])\n",
    "        # ====================================== #\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Scale Action**\n",
    "```python\n",
    "    def scale_action(self, action):\n",
    "        return torch.tensor([[action * ((self.action_range[1] - self.action_range[0]) / (self.num_of_action-1 )) + self.action_range[0]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Select Action**\n",
    "- select action bu using deterministic policy by using argument max the action value, and balance the exploration and exploitation  Learning with **$\\epsilon - greedy$** with probability to exploration with **$\\epsilon$**\n",
    "\n",
    "```python\n",
    "    def select_action(self, state):\n",
    "        \"\"\" Select an action based on an epsilon-greedy policy. \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self.num_of_action)\n",
    "        else:\n",
    "            # Exploitation: choose the action with the highest estimated Q-value\n",
    "            return np.argmax(self.q(state))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Decay Epsilon**\n",
    "- decaying epsilon to balancing exploration and exploitation\n",
    "\n",
    "```python\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\" Decay epsilon value to reduce exploration over time. \"\"\"\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon-self.epsilon_decay)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Linear Q Learning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Constructor**\n",
    "\n",
    "- initial Linear Q Learning class with updating parameter including\n",
    "    - Learning rate\n",
    "    - Initial Epsilon\n",
    "    - Epsilon Decay\n",
    "    - Final Epsilon\n",
    "    - Discount Factor\n",
    "    \n",
    "```python\n",
    "class Linear_QN(BaseAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_of_action: int = 2,\n",
    "            action_range: list = [-2.5, 2.5],\n",
    "            learning_rate: float = 0.01,\n",
    "            initial_epsilon: float = 1.0,\n",
    "            epsilon_decay: float = 1e-3,\n",
    "            final_epsilon: float = 0.001,\n",
    "            discount_factor: float = 0.95,\n",
    "    ) -> None:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Updating**\n",
    "- updating Linear Q Learning with using the gradient descent by using the gradient by using state\n",
    "- and error term using maximum action value from next state to set as target value, like a Q learning\n",
    "\n",
    "\n",
    "```python\n",
    "    def update(self,obs,action: int,reward: float,next_obs,next_action: int,terminated: bool\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Updates the weight vector using the Temporal Difference (TD) error \n",
    "        in Q-learning with linear function approximation.\n",
    "        \"\"\"\n",
    "        # ========= put your code here ========= #\n",
    "        q_curr = self.q(obs=obs, a=action)\n",
    "        if terminated:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.discount_factor * np.max(self.q(next_obs))\n",
    "        pass\n",
    "    \n",
    "        error = target - q_curr\n",
    "        self.training_error.append(error)\n",
    "        # Gradient descent update\n",
    "        self.w[:, action] += self.lr * error * obs['policy'][0].detach().cpu().numpy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Learn**\n",
    "- Set the function to make agent learning with environment by updating every timestep by using observation term as gradient\n",
    "\n",
    "```python\n",
    "    def learn(self, env):\n",
    "        \"\"\"\n",
    "        Train the agent on a single step.\n",
    "        \"\"\"\n",
    "        obs, _ = env.reset()\n",
    "        cumulative_reward = 0.0\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = self.select_action(obs)\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(self.scale_action(action))\n",
    "            reward_value = reward.item()\n",
    "            terminated_value = terminated.item() \n",
    "            cumulative_reward += reward_value\n",
    "            done = terminated or truncated\n",
    "            self.update(\n",
    "                obs=obs,\n",
    "                action=action,\n",
    "                reward=reward_value,\n",
    "                next_obs=next_obs,\n",
    "                next_action=action,\n",
    "                terminated=terminated_value\n",
    "            )\n",
    "            done = terminated or truncated\n",
    "            obs = next_obs\n",
    "            step += 1\n",
    "        self.decay_epsilon()\n",
    "        return cumulative_reward , step\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Q Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Neural Network**\n",
    "- setup neural network to approximate action value from policy\n",
    "- this neural consist with 1 hidden layer with fully connected layer\n",
    "- and forward fucntion to approximate  \n",
    "\n",
    "```python\n",
    "\n",
    "class DQN_network(nn.Module):\n",
    "    \"\"\" Neural network model for the Deep Q-Network algorithm. \"\"\"\n",
    "    def __init__(self, n_observations, hidden_size, n_actions, dropout):\n",
    "        super(DQN_network, self).__init__()\n",
    "        # ========= put your code here ========= #\n",
    "        self.fc1 = nn.Linear(n_observations, hidden_size) # Input layer\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions) # hidden layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through the network.\"\"\"\n",
    "        val = x\n",
    "        val = F.relu(self.fc1(val))\n",
    "        val = self.dropout(val)\n",
    "        val = F.relu(self.fc2(val))\n",
    "        val = self.dropout(val)\n",
    "\n",
    "        return val\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Constructor**\n",
    "\n",
    "- initial variable for Deep Q Network\n",
    "    - `tau` : constant for soft update in target network\n",
    "    - `hidden_dim` : number of neuron in hidden layer\n",
    "    - `learning_rate` : learning rate to updating gradient\n",
    "    - `dropout` : probability to black out neuron\n",
    "    - `buffer_size` : buffer size to collect experience\n",
    "    - `batch_size` : number of sampling to use to updating network \n",
    "    \n",
    "```python\n",
    "class DQN(BaseAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            device = None,\n",
    "            num_of_action: int = 2,\n",
    "            action_range: list = [-2.5, 2.5],\n",
    "            n_observations: int = 4,\n",
    "            hidden_dim: int = 64,\n",
    "            dropout: float = 0.5,\n",
    "            learning_rate: float = 0.005,\n",
    "            tau: float = 0.005,\n",
    "            initial_epsilon: float = 1.0,\n",
    "            epsilon_decay: float = 1e-3,\n",
    "            final_epsilon: float = 0.001,\n",
    "            discount_factor: float = 0.95,\n",
    "            buffer_size: int = 1000,\n",
    "            batch_size: int = 1,\n",
    "    ) -> None:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **Calculate Loss**\n",
    "\n",
    "-  Calculate DQN loss with following the equation\n",
    "\n",
    "$$\n",
    "L = (y_j +\\gamma \\max_{a'}Q(\\phi_{j+1} , a' ; \\theta))^2\n",
    "$$\n",
    "\n",
    "```python\n",
    "    def calculate_loss(self, non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch):\n",
    "        \"\"\" Computes the loss for policy optimization. \"\"\"\n",
    "        q = self.policy_net(state_batch).gather(1, action_batch) # [batch_size, 1]\n",
    "        q_next = torch.zeros(size=(self.batch_size , self.num_of_action), device=self.device)\n",
    "        if non_final_next_states.size(0) > 0:\n",
    "            with torch.no_grad():\n",
    "                q_next_values = self.target_net(non_final_next_states).detach()\n",
    "                q_next[non_final_mask.squeeze()] = q_next_values # Define Next Q value from next state , squeeze make dimension [batch_size , 1] to [batch_size]\n",
    "        q_expected = (torch.max(q_next , dim=1)[0].unsqueeze(1) * self.discount_factor) + reward_batch # Find Maximum Q Value over action : Dimension\n",
    "        loss = F.mse_loss(target=q_expected,input=q) # tensor(0.6990, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
    "        return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Generate Sample**\n",
    "- generate random sample(contain with state transition) with number of batch size to used for updating  \n",
    "\n",
    "```python\n",
    "    def generate_sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generates a batch sample from memory for training.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing:\n",
    "                - non_final_mask (Tensor): A boolean mask indicating which states are non-final.\n",
    "                - non_final_next_states (Tensor): The next states that are not terminal.\n",
    "                - state_batch (Tensor): The batch of current states.\n",
    "                - action_batch (Tensor): The batch of actions taken.\n",
    "                - reward_batch (Tensor): The batch of rewards received.\n",
    "        \"\"\"\n",
    "        # Ensure there are enough samples in memory before proceeding\n",
    "        # sample for training with batch size\n",
    "        if len(self.memory) < batch_size:\n",
    "            return None\n",
    "        batch = self.memory.sample()         \n",
    "        # ========= put your code here ========= #)\n",
    "        state_batch = torch.stack([torch.tensor(batch[i].state, dtype=torch.float) for i in range(self.batch_size)]).to(self.device)\n",
    "        next_states_batch = torch.stack([torch.tensor(batch[i].next_state, dtype=torch.float) for i in range(self.batch_size)]).to(self.device)\n",
    "        action_batch = torch.stack([torch.tensor(batch[i].action, dtype=torch.int64) for i in range(self.batch_size)]).to(self.device)\n",
    "        reward_batch = torch.stack([torch.tensor(batch[i].reward, dtype=torch.float) for i in range(self.batch_size)]).to(self.device)\n",
    "        non_final_mask = torch.stack([torch.tensor(not batch[i].done, dtype=torch.bool) for i in range(self.batch_size)]).to(self.device)\n",
    "        non_final_next_states = next_states_batch[non_final_mask]\n",
    "        # Return All dimension : [batch_size , 1]\n",
    "        return (non_final_mask.unsqueeze(1), non_final_next_states.squeeze(1), state_batch.squeeze(1), action_batch, reward_batch.unsqueeze(1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Update Policy Network**\n",
    "- updating policy network using gradient descest by using calculated loss to step the policy\n",
    "\n",
    "```python\n",
    "    def update_policy(self):\n",
    "        if self.memory.__len__() < self.batch_size:\n",
    "            return\n",
    "        sample = self.generate_sample(self.batch_size)\n",
    "        if sample is None:\n",
    "            return\n",
    "        non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch = sample\n",
    "        loss = self.calculate_loss(non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch) # tensor(0.7219, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Update Target Network**\n",
    "- updating target network with soft updating to make target network not correlate to policy network , we control ratio of policy network and target network weight\n",
    "\n",
    "```python\n",
    "    def update_target_networks(self):\n",
    "        target_net_state_dict = self.target_net.state_dict() # get target network weights\n",
    "        policy_net_state_dict = self.policy_net.state_dict()\n",
    "        for key in target_net_state_dict:\n",
    "            target_net_state_dict[key] = self.tau * policy_net_state_dict[key] + (1.0 - self.tau) * target_net_state_dict[key]\n",
    "        self.target_net.load_state_dict(target_net_state_dict)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MC REINFORCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Neural Network**\n",
    "\n",
    "```python\n",
    "class MC_REINFORCE_network(nn.Module):\n",
    "    \"\"\" Neural network for the MC_REINFORCE algorithm. \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations, hidden_size, n_actions, dropout):\n",
    "        super(MC_REINFORCE_network, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_observations, hidden_size) # Input layer\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions) # hidden layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through the network. \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Constructor**\n",
    "- initial value in MC_REINFORCE class most variable is same as Linear Q Learning\n",
    "\n",
    "```python\n",
    "class MC_REINFORCE(BaseAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            device = None,\n",
    "            num_of_action: int = 2,\n",
    "            action_range: list = [-2.5, 2.5],\n",
    "            n_observations: int = 4,\n",
    "            hidden_dim: int = 64,\n",
    "            dropout: float = 0.5,\n",
    "            learning_rate: float = 0.01,\n",
    "            discount_factor: float = 0.95,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the CartPole Agent.\n",
    "        \"\"\"     \n",
    "        self.LR = learning_rate\n",
    "\n",
    "        self.policy_net = MC_REINFORCE_network(n_observations, hidden_dim, num_of_action, dropout).to(device)\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=learning_rate)\n",
    "        self.device = device\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "        super(MC_REINFORCE, self).__init__(\n",
    "            num_of_action=num_of_action,\n",
    "            action_range=action_range,\n",
    "            learning_rate=learning_rate,\n",
    "            discount_factor=discount_factor,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Calculate Return**\n",
    "- calculate return from reward and discount from discount factor\n",
    "```python\n",
    "    def calculate_stepwise_returns(self, rewards):\n",
    "        \"\"\"\n",
    "        Compute stepwise returns for the trajectory.\n",
    "\n",
    "        Args:\n",
    "            rewards (list): List of rewards obtained in the episode.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Normalized stepwise returns. # Dim = [1]\n",
    "        \"\"\"\n",
    "        stepwise_return = 0\n",
    "        stepwise_return_arr = []\n",
    "        for r in reversed(rewards):\n",
    "            stepwise_return = stepwise_return*self.discount_factor + r\n",
    "            stepwise_return_arr.append(stepwise_return)\n",
    "        tensor_norm = F.normalize(input=torch.tensor(list(reversed(stepwise_return_arr))),dim=0)\n",
    "        return tensor_norm.tolist() # > tensor([-0.1740, -0.1021, 0.3525,  0.4109,  0.4675,  0.5201])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Calculate Return**\n",
    "- Generate trajectory to create sample for update\n",
    "```python\n",
    "    def generate_trajectory(self, env):\n",
    "        \"\"\"\n",
    "        Generate a trajectory by interacting with the environment.\n",
    "\n",
    "        Args:\n",
    "            env: The environment object.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple: (timestep ,episode_return, stepwise_returns, log_prob_actions, trajectory)\n",
    "        \"\"\"\n",
    "        # ===== Initialize trajectory collection variables ===== #\n",
    "        # Reset environment to get initial state (tensor)\n",
    "        # Store state-action-reward history (list)\n",
    "        # Store log probabilities of actions (list)\n",
    "        # Store rewards at each step (list)\n",
    "        # Track total episode return (float)\n",
    "        # Flag to indicate episode termination (boolean)\n",
    "        # Step counter (int)\n",
    "        # ========= put your code here ========= #\n",
    "        obs , _  = env.reset()\n",
    "        state_hist = []\n",
    "        reward_hist = []\n",
    "        action_hist = []\n",
    "        log_prob_action_hist = []\n",
    "        episode_return_hist = 0\n",
    "        timestep = 0\n",
    "        cumulative_reward = 0\n",
    "        done = False\n",
    "        # ====================================== #\n",
    "        \n",
    "        # ===== Collect trajectory through agent-environment interaction ===== #\n",
    "        # In Episode\n",
    "        while not done:\n",
    "            \n",
    "            # Predict action from the policy network\n",
    "            # State into policy to return probability of each action\n",
    "            prob_each_action = self.policy_net(obs['policy']) # > tensor([[0.1380, 0.1534, 0.1328, 0.1328, 0.1656, 0.1328, 0.1446]],device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
    "            # Change to Probability Distribution\n",
    "            prob_cat = torch.distributions.Categorical(prob_each_action) # > Categorical(probs: torch.Size([1, 7]))\n",
    "            action_idx = prob_cat.sample() # > tensor([1], device='cuda:0')\n",
    "\n",
    "            # Execute action in the environment and observe next state and reward\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(self.scale_action(action_idx))  # Step Environment\n",
    "            reward_value = reward.item() # > int : 1\n",
    "            terminated_value = terminated.item() \n",
    "            cumulative_reward += reward_value\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Store action log probability reward and trajectory history\n",
    "            reward_hist.append(reward_value)\n",
    "            state_hist.append(obs)\n",
    "            log_prob_action_hist.append(prob_cat.log_prob(action_idx)) # Collect in list and reduce dimension and change to list\n",
    "            \n",
    "            # Update state\n",
    "            obs = next_obs\n",
    "            timestep += 1\n",
    "            if done:\n",
    "                self.plot_durations(timestep)\n",
    "                break\n",
    "\n",
    "        # ===== Stack log_prob_actions &  stepwise_returns ===== #\n",
    "        stepwise_returns = self.calculate_stepwise_returns(rewards=reward_hist)\n",
    " \n",
    "        self.episode_durations.append(timestep)\n",
    "        self.rewards.append(cumulative_reward)\n",
    "        return (cumulative_reward , stepwise_returns , log_prob_action_hist , state_hist)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calculating Loss**\n",
    "```python\n",
    "    def calculate_loss(self, stepwise_returns, log_prob_actions):\n",
    "        \"\"\"\n",
    "        Compute the loss for policy optimization.\n",
    "        Args:\n",
    "            stepwise_returns (List): Stepwise returns for the trajectory. : Dim list = [n]\n",
    "            log_prob_actions (tensor): Log probabilities of actions taken. : Dim list = [n] : n is tensor contain with prob\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Computed loss.\n",
    "        \"\"\"\n",
    "        log_probs = torch.stack(log_prob_actions).flatten()\n",
    "        # print(log_probs.shape)\n",
    "        # loss = -torch.sum((log_probs * stepwise_returns))\n",
    "        loss = -(log_probs * stepwise_returns).mean()\n",
    "        return loss # > tensor(2.5966) : Scalar\n",
    "```\n",
    "#### **Updating Policy**\n",
    "\n",
    "```python\n",
    "    def update_policy(self, stepwise_returns, log_prob_actions):\n",
    "        \"\"\"\n",
    "        Update the policy using the calculated loss.\n",
    "\n",
    "        Args:\n",
    "            stepwise_returns (Tensor): Stepwise returns.\n",
    "            log_prob_actions (Tensor): Log probabilities of actions taken.\n",
    "        \n",
    "        Returns:\n",
    "            float: Loss value after the update.\n",
    "        \"\"\"\n",
    "        loss = self.calculate_loss(stepwise_returns=stepwise_returns , log_prob_actions=log_prob_actions) # get tensor loss value\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PPO**\n",
    "```python\n",
    "import random\n",
    "import os\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.nn.functional import mse_loss\n",
    "from RL_Algorithm.RL_base_function import BaseAlgorithm\n",
    "\n",
    "class RolloutBuffer():\n",
    "    def __init__(self , buffer_size , n_envs):\n",
    "        self.n_envs = n_envs\n",
    "        self.buffer_size = buffer_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.advantages = torch.tensor((self.buffer_size, self.n_envs), dtype=torch.float32)\n",
    "    def add(self, state, action, reward, log_prob, values, done):\n",
    "        # Detach to avoid carrying the computation graph\n",
    "        self.memory.append((\n",
    "            state.detach(), \n",
    "            action.detach(), \n",
    "            reward.detach(), \n",
    "            log_prob.detach(),\n",
    "            values.detach(), \n",
    "            done.detach() if isinstance(done, torch.Tensor) else done\n",
    "        ))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def sample_all_env(self , batch_size:int):\n",
    "        '''\n",
    "        Return random Transition with number of batch size from all environment \n",
    "        '''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def sample_batch(self , batch_size:int):\n",
    "\n",
    "        states, actions, rewards, log_probs_old, values, dones = zip(*self.memory)\n",
    "        \n",
    "        states        = torch.cat(states, dim=0) # > change to tensor\n",
    "        actions       = torch.cat(actions, dim=0)\n",
    "        rewards       = torch.cat(rewards, dim=0)\n",
    "        log_probs_old = torch.cat(log_probs_old , dim=0)\n",
    "        values        = torch.cat(values, dim=0)\n",
    "        dones         = torch.cat(dones, dim=0)\n",
    "        advantages    = self.advantages.flatten()\n",
    "\n",
    "        random_indices = torch.randperm(len(states))[:batch_size]\n",
    "        return states[random_indices] , actions[random_indices] , rewards[random_indices], log_probs_old[random_indices] , values[random_indices] , dones[random_indices] , advantages[random_indices]\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=1e-4):\n",
    "        \"\"\"\n",
    "        Actor network for policy approximation.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimension of the state space.\n",
    "            hidden_dim (int): Number of hidden units in layers.\n",
    "            output_dim (int): Dimension of the action space.\n",
    "            learning_rate (float, optional): Learning rate for optimization. Defaults to 1e-4.\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) # Input to hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim) # hidden to hidden layer\n",
    "        \n",
    "        self.actor_head = nn.Linear(hidden_dim, output_dim) # hidden layer\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize network weights using Xavier initialization for better convergence.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)  # Xavier initialization\n",
    "                nn.init.zeros_(m.bias)  # Initialize bias to 0\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass for action selection.\n",
    "\n",
    "        Args:\n",
    "            state (Tensor): Current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Selected action values.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        actor_out = F.relu(self.actor_head(x))\n",
    "        actor_prob = self.softmax(actor_out)\n",
    "\n",
    "        return actor_prob\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, learning_rate=1e-4):\n",
    "        \"\"\"\n",
    "        Actor network for policy approximation.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimension of the state space.\n",
    "            hidden_dim (int): Number of hidden units in layers.\n",
    "            output_dim (int): Dimension of the action space.\n",
    "            learning_rate (float, optional): Learning rate for optimization. Defaults to 1e-4.\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) # Input to hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim) # hidden to hidden layer\n",
    "        \n",
    "        self.critic_head = nn.Linear(hidden_dim , 1)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize network weights using Xavier initialization for better convergence.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)  # Xavier initialization\n",
    "                nn.init.zeros_(m.bias)  # Initialize bias to 0\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass for action selection.\n",
    "\n",
    "        Args:\n",
    "            state (Tensor): Current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Selected action values.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        critic_out = self.critic_head(x)\n",
    "\n",
    "        return critic_out\n",
    "class PPO(BaseAlgorithm):\n",
    "    def __init__(self, \n",
    "                device = None, \n",
    "                num_of_action: int = 2,\n",
    "                action_range: list = [-2.5, 2.5],\n",
    "                n_observations: int = 4,\n",
    "                hidden_dim = 256,\n",
    "                dropout = 0.05, \n",
    "                learning_rate: float = 0.01,\n",
    "                buffer_size: int = 256,\n",
    "                batch_size: int = 1,\n",
    "                discount_factor: float = 0.95,\n",
    "                lamda : float = 1,\n",
    "                nun_envs : int = 1,\n",
    "                eps_clip : float = 0.2,\n",
    "                critic_loss_coeff : float = 0.5,\n",
    "                entropy_loss_coeff : float = 0.1,\n",
    "                epoch : int = 20\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Actor-Critic algorithm implementation.\n",
    "\n",
    "        Args:\n",
    "            device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "            num_of_action (int, optional): Number of possible actions. Defaults to 2.\n",
    "            action_range (list, optional): Range of action values. Defaults to [-2.5, 2.5].\n",
    "            n_observations (int, optional): Number of observations in state. Defaults to 4.\n",
    "            hidden_dim (int, optional): Hidden layer dimension. Defaults to 256.\n",
    "            learning_rate (float, optional): Learning rate. Defaults to 0.01.\n",
    "            tau (float, optional): Soft update parameter. Defaults to 0.005.\n",
    "            discount_factor (float, optional): Discount factor for Q-learning. Defaults to 0.95.\n",
    "            batch_size (int, optional): Size of training batches. Defaults to 1.\n",
    "            buffer_size (int, optional): Replay buffer size. Defaults to 256.\n",
    "        \"\"\"\n",
    "        # Feel free to add or modify any of the initialized variables above.\n",
    "        # ========= put your code here ========= #\n",
    "        self.device = device\n",
    "        self.actor = Actor(n_observations, hidden_dim, num_of_action, learning_rate).to(device)\n",
    "        self.critic = Critic(n_observations, hidden_dim, learning_rate).to(device)\n",
    "        self.batch_size = batch_size\n",
    "        self.lamda = lamda\n",
    "        self.rollout_buffer = RolloutBuffer(buffer_size=buffer_size , n_envs =nun_envs)\n",
    "        self.discount_factor = discount_factor\n",
    "        self.eps_clip = eps_clip\n",
    "        self.num_envs = nun_envs\n",
    "        self.critic_loss_coeff = critic_loss_coeff\n",
    "        self.entropy_loss_coeff = entropy_loss_coeff\n",
    "        self.epoch = epoch\n",
    "        self.optimizer = optim.AdamW(list(self.actor.parameters()) + list(self.critic.parameters()), lr=learning_rate, amsgrad=True)\n",
    "\n",
    "        # Experiment with different values and configurations to see how they affect the training process.\n",
    "        # Remember to document any changes you make and analyze their impact on the agent's performance.\n",
    "\n",
    "        pass\n",
    "        # ====================================== #\n",
    "\n",
    "        super(PPO, self).__init__(\n",
    "            num_of_action=num_of_action,\n",
    "            action_range=action_range,\n",
    "            learning_rate=learning_rate,\n",
    "            buffer_size=buffer_size,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        # set up matplotlib\n",
    "        self.is_ipython = 'inline' in matplotlib.get_backend()\n",
    "        if self.is_ipython:\n",
    "            from IPython import display\n",
    "\n",
    "    def select_action(self, prob_each_action, noise=0.0) -> int:\n",
    "        \"\"\"\n",
    "        Selects an action based on the current policy with optional exploration noise.\n",
    "        \n",
    "        Args:\n",
    "        state (Tensor): The current state of the environment. [[n1,n2,n3,n4,..nn]]\n",
    "        noise (float, optional): The standard deviation of noise for exploration. Defaults to 0.0.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: \n",
    "                - scaled_action: The final action after scaling.\n",
    "            Tensor:\n",
    "                - Probabiblity from action : dim : tensor([n])\n",
    "                - Log probability of the action taken.\n",
    "                - Entropy of the action distribution.\n",
    "        \"\"\"\n",
    "        # Change to Probability Distribution\n",
    "        dist = torch.distributions.Categorical(prob_each_action) # > Categorical(probs: torch.Size([1, 7]))\n",
    "        action_idx = dist.sample() # > tensor([1], device='cuda:0')\n",
    "        action_prob = dist.probs.gather(1, action_idx.unsqueeze(1)).squeeze(1)  # shape: [num_env]\n",
    "        log_prob = dist.log_prob(action_idx) \n",
    "        entropy = dist.entropy()\n",
    "        # [num_env] , [num_env , num_action] , [num_env , num_action] , [num_env] \n",
    "        return action_idx , action_prob ,log_prob , entropy \n",
    "\n",
    "    def scale_action(self, action):\n",
    "        \"\"\"\n",
    "        Maps a discrete action in range [0, n] to a continuous value in [action_min, action_max].\n",
    "\n",
    "        Args:\n",
    "            action (int): Discrete action in range [0, n].\n",
    "            n (int): Number of discrete actions (inclusive range from 0 to n).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Scaled action tensor.\n",
    "        \"\"\"\n",
    "        # ========= put your code here ========= #\n",
    "        # print(\"----------------\")\n",
    "        # print(action)\n",
    "        scale_factor = (self.action_range[1] - self.action_range[0]) / (self.num_of_action-1 )\n",
    "        scaled_action = action * scale_factor + self.action_range[0]\n",
    "        return scaled_action.view(-1, 1) \n",
    "    \n",
    "    def update_policy(self , memory):\n",
    "        \"\"\"\n",
    "        Update the policy using the calculated loss.\n",
    "\n",
    "        Returns:\n",
    "            float: Loss value after the update.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, log_probs_old, values, dones , advantages = memory\n",
    "        # for _ in range(self.epoch):\n",
    "        values = self.critic(states).squeeze(-1) # > (batch size * num_env , 1)\n",
    "        # advantage = self.calculate_advantage(rewards , dones , values.squeeze()) # > [] , [] , []\n",
    "        \n",
    "        values = (values-values.mean())/(values.std()+1e-8)\n",
    "\n",
    "        returns = advantages + values\n",
    "\n",
    "        probs = self.actor(states)                  # Get new action probabilities.\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        log_probs_new = dist.log_prob(actions.squeeze())\n",
    "\n",
    "        # Actor Loss\n",
    "        ratio = torch.exp(log_probs_new - log_probs_old)\n",
    "        surr1 = ratio*advantages\n",
    "        surr2 = torch.clamp(ratio , 1.0-self.eps_clip , 1.0+self.eps_clip)*advantages\n",
    "        actor_loss = -torch.min(surr1,surr2).mean()\n",
    "\n",
    "        # Critic Loss\n",
    "        critic_loss = F.mse_loss(values, returns)\n",
    "\n",
    "        # Entropy bonus\n",
    "        entrupy_bonus = dist.entropy().mean()\n",
    "\n",
    "        # Final Loss\n",
    "        loss = actor_loss + self.critic_loss_coeff*critic_loss + self.entropy_loss_coeff * entrupy_bonus\n",
    "        # Perform backpropagation and optimizer step.\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Optionally clip gradients here if needed.\n",
    "        self.optimizer.step()\n",
    "        return loss.item() , actor_loss.item() , critic_loss.item() , entrupy_bonus.item()\n",
    "\n",
    "    def calculate_advantage(self , rewards , dones , last_values):\n",
    "        states, actions, rewards, log_probs_old, values , dones = zip(*self.rollout_buffer.memory)\n",
    "        # Convert to tensors\n",
    "        rewards = torch.stack(rewards).to(self.device)\n",
    "        values = torch.stack(values).to(self.device).squeeze(2)\n",
    "        dones = torch.stack(dones).to(self.device).float()\n",
    "        last_values = last_values.flatten()\n",
    "        \n",
    "        # Dimension : [buffer_size , number_envs]\n",
    "        # torch.Size([500, 64])\n",
    "        # torch.Size([500, 64])\n",
    "        # torch.Size([500, 64])\n",
    "        # torch.Size([64])\n",
    "\n",
    "        T_step, n_envs = rewards.shape\n",
    "        advantages = torch.zeros((T_step, n_envs), dtype=torch.float32, device=self.device)\n",
    "        gae = torch.zeros(n_envs, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # print(\"debug.........\")\n",
    "\n",
    "        gae = torch.zeros(n_envs, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        for t in reversed(range(T_step)):\n",
    "            mask = 1.0 - dones[t]  # [n_envs]\n",
    "            next_value = last_values if t == T_step - 1 else values[t + 1]  # [n_envs]\n",
    "            delta = rewards[t] + self.discount_factor * next_value * mask - values[t]\n",
    "            gae = delta + self.discount_factor * self.lamda * mask * gae\n",
    "            advantages[t] = gae\n",
    "        advantages = (advantages - advantages.mean())/(advantages.std() + 1e-8)\n",
    "        self.rollout_buffer.advantages = advantages\n",
    "\n",
    "    \n",
    "    # def compute_returns_and_advantage(self , last_values : torch.Tensor , dones : np.ndarray) -> None:\n",
    "    #     last_values = last_values.clone().cpu().numpy().flatten()\n",
    "    #     dones = dones.cpu().numpy()\n",
    "    #     last_gae_lam = 0\n",
    "    #     for step in reversed(range(self.buffer_size)):\n",
    "    #         if step == self.buffer_size - 1: # Use real last value\n",
    "    #             next_non_terminal = 1.0 - dones.astype(np.float32)\n",
    "    #             next_values = last_values\n",
    "    #         else:\n",
    "    #             next_non_terminal = 1.0 - self.episode_starts[step + 1]\n",
    "    #             next_values = self.values[step + 1]\n",
    "    #         delta = self.rewards[step] + self.gamma * next_values * next_non_terminal - self.values[step]\n",
    "    #         last_gae_lam = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae_lam\n",
    "    #         self.advantages[step] = last_gae_lam\n",
    "    #         # print(\"debugging.....\")\n",
    "\n",
    "    def train(self , env , max_steps = 1000):\n",
    "        obs , _  = env.reset()\n",
    "        num_envs = obs['policy'].shape[0]\n",
    "\n",
    "        steps_per_env = torch.zeros(num_envs, dtype=torch.int, device=obs['policy'].device)\n",
    "        cumulative_reward_per_env = torch.zeros(num_envs, dtype=torch.float, device=self.device)\n",
    "\n",
    "        time_step_buffer = deque(maxlen=10)\n",
    "        reward_buffer = deque(maxlen=10)\n",
    "\n",
    "        reward_avg = 0\n",
    "        time_avg = 0\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        cumulative_reward = 0\n",
    "        done = False\n",
    "        # ====================================== #\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Predict action from the policy network\n",
    "            prob_each_action = self.actor(obs['policy']) \n",
    "            action_idx , action_prob , log_prob , entropy  = self.select_action(prob_each_action=prob_each_action) # > tensor([4], device='cuda:0')\n",
    "            values = self.critic(obs['policy'])\n",
    "            # Execute action in the environment and observe next state and reward\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(self.scale_action(action_idx))  # Step Environmentscripts/Function_based/train.py --task Stabilize-Isaac-Cartpole-v0 \n",
    "            done = torch.logical_or(terminated, truncated)\n",
    "            # Store the transition in memory\n",
    "            self.rollout_buffer.add(state=obs['policy'],action=action_idx,reward=reward,log_prob=log_prob,values=values,done=done)\n",
    "            \n",
    "\n",
    "            # ====================================== #\n",
    "\n",
    "            # Update state\n",
    "            obs = next_obs\n",
    "            active_envs = torch.logical_not(done)\n",
    "\n",
    "            steps_per_env[active_envs] += 1\n",
    "            done_idx = torch.where(done)[0]\n",
    "\n",
    "            cumulative_reward_per_env += reward\n",
    "            for index in done_idx:\n",
    "                time_step_buffer.append(steps_per_env[index].item())\n",
    "                reward_buffer.append(cumulative_reward_per_env[index].item())\n",
    "                reward_avg = torch.mean(torch.tensor(reward_buffer, dtype=torch.float))\n",
    "                time_avg = torch.mean(torch.tensor(time_step_buffer , dtype=torch.float))\n",
    "\n",
    "            steps_per_env[done_idx] = 0\n",
    "            cumulative_reward_per_env[done_idx] = 0\n",
    "\n",
    "        last_val = self.critic(obs['policy'])\n",
    "        advantage = self.calculate_advantage(reward, dones=done , last_values=last_val) # > [] , [] , []\n",
    "        memory = self.rollout_buffer.sample_batch(self.batch_size)\n",
    "        loss , actor_loss , critic_loss , entropy_bonus = self.update_policy(memory=memory)    \n",
    "        # print(\"UPDATING POLICY!! ก'w'ก\")\n",
    "\n",
    "        # reward = 0\n",
    "        # time_avg = 0\n",
    "        # loss = 0\n",
    "        return reward_avg , time_avg , loss , actor_loss , critic_loss , entropy_bonus\n",
    "\n",
    "    def learn(self, env, max_steps=1000):\n",
    "        \"\"\"\n",
    "        Train the agent on a single step.\n",
    "\n",
    "        Args:\n",
    "            env: The environment in which the agent interacts.\n",
    "            max_steps (int): Maximum number of steps per episode.\n",
    "            num_agents (int): Number of agents in the environment.\n",
    "            noise_scale (float, optional): Initial exploration noise level. Defaults to 0.1.\n",
    "            noise_decay (float, optional): Factor by which noise decreases per step. Defaults to 0.99.\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== Initialize trajectory collection variables ===== #\n",
    "        # Reset environment to get initial state (tensor)\n",
    "        # Track total episode return (float)\n",
    "        # Flag to indicate episode termination (boolean)\n",
    "        # Step counter (int)\n",
    "        # ========= put your code here ========= #\n",
    "        reward_avg , timestep_avg , loss , actor_loss , critic_loss , entropy_bonus = self.train(env=env , max_steps=max_steps)\n",
    "        self.training_error.append(loss)\n",
    "\n",
    "        return reward_avg , timestep_avg , loss , actor_loss , critic_loss , entropy_bonus\n",
    "        # self.plot_durations(timestep_avg)\n",
    "\n",
    "    def save_net_weights(self, path, filename):\n",
    "        \"\"\"\n",
    "        Save weight parameters.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        filepath = os.path.join(path, filename)\n",
    "        torch.save({\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic_state_dict': self.critic.state_dict(),\n",
    "        }, filepath)\n",
    "        \n",
    "    def load_net_weights(self, path, filename):\n",
    "        \"\"\"\n",
    "        Load weight parameters.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(os.path.join(path, filename))\n",
    "        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "\n",
    "    # ================================================================================== #\n",
    "    def plot_durations(self, timestep=None, show_result=False):\n",
    "        if timestep is not None:\n",
    "            self.episode_durations.append(timestep)\n",
    "\n",
    "        plt.figure(1)\n",
    "        durations_t = torch.tensor(self.episode_durations, dtype=torch.float)\n",
    "        if show_result:\n",
    "            plt.title('Result')\n",
    "        else:\n",
    "            plt.clf()\n",
    "            plt.title('Training...')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Duration')\n",
    "        plt.plot(durations_t.numpy())\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(durations_t) >= 100:\n",
    "            means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(99), means))\n",
    "            plt.plot(means.numpy())\n",
    "\n",
    "        plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "        if self.is_ipython:\n",
    "            if not show_result:\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            else:\n",
    "                display.display(plt.gcf())\n",
    "    # ================================================================================== #\n",
    "```\n",
    "\n",
    "- PPO must add some rollout buffer for "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
