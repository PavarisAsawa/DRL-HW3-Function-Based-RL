{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 2: Setting up `Cart-Pole` Agent.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`Name`** : **Pavaris Asawakijtananont**\n",
    "\n",
    "- **`Number`** : **65340500037**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Base Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **q**\n",
    "- calling the action value with using linear approximator to use with Linear Q Learning \n",
    "\n",
    "```python\n",
    "    def q(self, obs, a=None):\n",
    "        \"\"\"Returns the linearly-estimated Q-value for a given state and action.\"\"\"\n",
    "        obs_val = obs['policy'][0].detach().cpu().numpy()\n",
    "        if a==None:\n",
    "            # Get q values from all action in state\n",
    "            return np.dot(obs_val, self.w)\n",
    "        else:\n",
    "            # Get q values given action & state\n",
    "            return np.dot(obs_val, self.w[:, a])\n",
    "        # ====================================== #\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Scale Action**\n",
    "```python\n",
    "    def scale_action(self, action):\n",
    "        return torch.tensor([[action * ((self.action_range[1] - self.action_range[0]) / (self.num_of_action-1 )) + self.action_range[0]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Select Action**\n",
    "- select action bu using deterministic policy by using argument max the action value, and balance the exploration and exploitation  Learning with **$\\epsilon - greedy$** with probability to exploration with **$\\epsilon$**\n",
    "\n",
    "```python\n",
    "    def select_action(self, state):\n",
    "        \"\"\" Select an action based on an epsilon-greedy policy. \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self.num_of_action)\n",
    "        else:\n",
    "            # Exploitation: choose the action with the highest estimated Q-value\n",
    "            return np.argmax(self.q(state))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Decay Epsilon**\n",
    "- decaying epsilon to balancing exploration and exploitation\n",
    "\n",
    "```python\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\" Decay epsilon value to reduce exploration over time. \"\"\"\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon-self.epsilon_decay)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Linear Q Learning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Constructor**\n",
    "\n",
    "- initial Linear Q Learning class with updating parameter including\n",
    "    - Learning rate\n",
    "    - Initial Epsilon\n",
    "    - Epsilon Decay\n",
    "    - Final Epsilon\n",
    "    - Discount Factor\n",
    "    \n",
    "```python\n",
    "class Linear_QN(BaseAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_of_action: int = 2,\n",
    "            action_range: list = [-2.5, 2.5],\n",
    "            learning_rate: float = 0.01,\n",
    "            initial_epsilon: float = 1.0,\n",
    "            epsilon_decay: float = 1e-3,\n",
    "            final_epsilon: float = 0.001,\n",
    "            discount_factor: float = 0.95,\n",
    "    ) -> None:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Updating**\n",
    "- updating Linear Q Learning with using the gradient descent by using the gradient by using state\n",
    "- and error term using maximum action value from next state to set as target value, like a Q learning\n",
    "\n",
    "\n",
    "```python\n",
    "    def update(self,obs,action: int,reward: float,next_obs,next_action: int,terminated: bool\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Updates the weight vector using the Temporal Difference (TD) error \n",
    "        in Q-learning with linear function approximation.\n",
    "        \"\"\"\n",
    "        # ========= put your code here ========= #\n",
    "        q_curr = self.q(obs=obs, a=action)\n",
    "        if terminated:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.discount_factor * np.max(self.q(next_obs))\n",
    "        pass\n",
    "    \n",
    "        error = target - q_curr\n",
    "        self.training_error.append(error)\n",
    "        # Gradient descent update\n",
    "        self.w[:, action] += self.lr * error * obs['policy'][0].detach().cpu().numpy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Learn**\n",
    "- Set the function to make agent learning with environment by updating every timestep by using observation term as gradient\n",
    "\n",
    "```python\n",
    "    def learn(self, env):\n",
    "        \"\"\"\n",
    "        Train the agent on a single step.\n",
    "        \"\"\"\n",
    "        obs, _ = env.reset()\n",
    "        cumulative_reward = 0.0\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = self.select_action(obs)\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(self.scale_action(action))\n",
    "            reward_value = reward.item()\n",
    "            terminated_value = terminated.item() \n",
    "            cumulative_reward += reward_value\n",
    "            done = terminated or truncated\n",
    "            self.update(\n",
    "                obs=obs,\n",
    "                action=action,\n",
    "                reward=reward_value,\n",
    "                next_obs=next_obs,\n",
    "                next_action=action,\n",
    "                terminated=terminated_value\n",
    "            )\n",
    "            done = terminated or truncated\n",
    "            obs = next_obs\n",
    "            step += 1\n",
    "        self.decay_epsilon()\n",
    "        return cumulative_reward , step\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Q Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Neural Network**\n",
    "- setup neural network to approximate action value from policy\n",
    "- this neural consist with 1 hidden layer with fully connected layer\n",
    "- and forward fucntion to approximate  \n",
    "\n",
    "```python\n",
    "\n",
    "class DQN_network(nn.Module):\n",
    "    \"\"\" Neural network model for the Deep Q-Network algorithm. \"\"\"\n",
    "    def __init__(self, n_observations, hidden_size, n_actions, dropout):\n",
    "        super(DQN_network, self).__init__()\n",
    "        # ========= put your code here ========= #\n",
    "        self.fc1 = nn.Linear(n_observations, hidden_size) # Input layer\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions) # hidden layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through the network.\"\"\"\n",
    "        val = x\n",
    "        val = F.relu(self.fc1(val))\n",
    "        val = self.dropout(val)\n",
    "        val = F.relu(self.fc2(val))\n",
    "        val = self.dropout(val)\n",
    "\n",
    "        return val\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Constructor**\n",
    "\n",
    "- initial variable for Deep Q Network\n",
    "    - `tau` : constant for soft update in target network\n",
    "    - `hidden_dim` : number of neuron in hidden layer\n",
    "    - `learning_rate` : learning rate to updating gradient\n",
    "    - `dropout` : probability to black out neuron\n",
    "    - `buffer_size` : buffer size to collect experience\n",
    "    - `batch_size` : number of sampling to use to updating network \n",
    "    \n",
    "```python\n",
    "class DQN(BaseAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            device = None,\n",
    "            num_of_action: int = 2,\n",
    "            action_range: list = [-2.5, 2.5],\n",
    "            n_observations: int = 4,\n",
    "            hidden_dim: int = 64,\n",
    "            dropout: float = 0.5,\n",
    "            learning_rate: float = 0.005,\n",
    "            tau: float = 0.005,\n",
    "            initial_epsilon: float = 1.0,\n",
    "            epsilon_decay: float = 1e-3,\n",
    "            final_epsilon: float = 0.001,\n",
    "            discount_factor: float = 0.95,\n",
    "            buffer_size: int = 1000,\n",
    "            batch_size: int = 1,\n",
    "    ) -> None:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **Calculate Loss**\n",
    "\n",
    "-  Calculate DQN loss with following the equation\n",
    "\n",
    "$$\n",
    "L = (y_j +\\gamma \\max_{a'}Q(\\phi_{j+1} , a' ; \\theta))^2\n",
    "$$\n",
    "\n",
    "```python\n",
    "    def calculate_loss(self, non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch):\n",
    "        \"\"\" Computes the loss for policy optimization. \"\"\"\n",
    "        q = self.policy_net(state_batch).gather(1, action_batch) # [batch_size, 1]\n",
    "        q_next = torch.zeros(size=(self.batch_size , self.num_of_action), device=self.device)\n",
    "        if non_final_next_states.size(0) > 0:\n",
    "            with torch.no_grad():\n",
    "                q_next_values = self.target_net(non_final_next_states).detach()\n",
    "                q_next[non_final_mask.squeeze()] = q_next_values # Define Next Q value from next state , squeeze make dimension [batch_size , 1] to [batch_size]\n",
    "        q_expected = (torch.max(q_next , dim=1)[0].unsqueeze(1) * self.discount_factor) + reward_batch # Find Maximum Q Value over action : Dimension\n",
    "        loss = F.mse_loss(target=q_expected,input=q) # tensor(0.6990, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
    "        return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Generate Sample**\n",
    "- generate random sample(contain with state transition) with number of batch size to used for updating  \n",
    "\n",
    "```python\n",
    "    def generate_sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generates a batch sample from memory for training.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing:\n",
    "                - non_final_mask (Tensor): A boolean mask indicating which states are non-final.\n",
    "                - non_final_next_states (Tensor): The next states that are not terminal.\n",
    "                - state_batch (Tensor): The batch of current states.\n",
    "                - action_batch (Tensor): The batch of actions taken.\n",
    "                - reward_batch (Tensor): The batch of rewards received.\n",
    "        \"\"\"\n",
    "        # Ensure there are enough samples in memory before proceeding\n",
    "        # sample for training with batch size\n",
    "        if len(self.memory) < batch_size:\n",
    "            return None\n",
    "        batch = self.memory.sample()         \n",
    "        # ========= put your code here ========= #)\n",
    "        state_batch = torch.stack([torch.tensor(batch[i].state, dtype=torch.float) for i in range(self.batch_size)]).to(self.device)\n",
    "        next_states_batch = torch.stack([torch.tensor(batch[i].next_state, dtype=torch.float) for i in range(self.batch_size)]).to(self.device)\n",
    "        action_batch = torch.stack([torch.tensor(batch[i].action, dtype=torch.int64) for i in range(self.batch_size)]).to(self.device)\n",
    "        reward_batch = torch.stack([torch.tensor(batch[i].reward, dtype=torch.float) for i in range(self.batch_size)]).to(self.device)\n",
    "        non_final_mask = torch.stack([torch.tensor(not batch[i].done, dtype=torch.bool) for i in range(self.batch_size)]).to(self.device)\n",
    "        non_final_next_states = next_states_batch[non_final_mask]\n",
    "        # Return All dimension : [batch_size , 1]\n",
    "        return (non_final_mask.unsqueeze(1), non_final_next_states.squeeze(1), state_batch.squeeze(1), action_batch, reward_batch.unsqueeze(1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Update Policy Network**\n",
    "- updating policy network using gradient descest by using calculated loss to step the policy\n",
    "\n",
    "```python\n",
    "    def update_policy(self):\n",
    "        if self.memory.__len__() < self.batch_size:\n",
    "            return\n",
    "        sample = self.generate_sample(self.batch_size)\n",
    "        if sample is None:\n",
    "            return\n",
    "        non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch = sample\n",
    "        loss = self.calculate_loss(non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch) # tensor(0.7219, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Update Target Network**\n",
    "- updating target network with soft updating to make target network not correlate to policy network , we control ratio of policy network and target network weight\n",
    "\n",
    "```python\n",
    "    def update_target_networks(self):\n",
    "        target_net_state_dict = self.target_net.state_dict() # get target network weights\n",
    "        policy_net_state_dict = self.policy_net.state_dict()\n",
    "        for key in target_net_state_dict:\n",
    "            target_net_state_dict[key] = self.tau * policy_net_state_dict[key] + (1.0 - self.tau) * target_net_state_dict[key]\n",
    "        self.target_net.load_state_dict(target_net_state_dict)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MC REINFORCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Neural Network**\n",
    "\n",
    "```python\n",
    "class MC_REINFORCE_network(nn.Module):\n",
    "    \"\"\" Neural network for the MC_REINFORCE algorithm. \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations, hidden_size, n_actions, dropout):\n",
    "        super(MC_REINFORCE_network, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_observations, hidden_size) # Input layer\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions) # hidden layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through the network. \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Constructor**\n",
    "- initial value in MC_REINFORCE class most variable is same as Linear Q Learning\n",
    "\n",
    "```python\n",
    "class MC_REINFORCE(BaseAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            device = None,\n",
    "            num_of_action: int = 2,\n",
    "            action_range: list = [-2.5, 2.5],\n",
    "            n_observations: int = 4,\n",
    "            hidden_dim: int = 64,\n",
    "            dropout: float = 0.5,\n",
    "            learning_rate: float = 0.01,\n",
    "            discount_factor: float = 0.95,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the CartPole Agent.\n",
    "        \"\"\"     \n",
    "        self.LR = learning_rate\n",
    "\n",
    "        self.policy_net = MC_REINFORCE_network(n_observations, hidden_dim, num_of_action, dropout).to(device)\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=learning_rate)\n",
    "        self.device = device\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "        super(MC_REINFORCE, self).__init__(\n",
    "            num_of_action=num_of_action,\n",
    "            action_range=action_range,\n",
    "            learning_rate=learning_rate,\n",
    "            discount_factor=discount_factor,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Calculate Return**\n",
    "- calculate return from reward and discount from discount factor\n",
    "```python\n",
    "    def calculate_stepwise_returns(self, rewards):\n",
    "        \"\"\"\n",
    "        Compute stepwise returns for the trajectory.\n",
    "\n",
    "        Args:\n",
    "            rewards (list): List of rewards obtained in the episode.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Normalized stepwise returns. # Dim = [1]\n",
    "        \"\"\"\n",
    "        stepwise_return = 0\n",
    "        stepwise_return_arr = []\n",
    "        for r in reversed(rewards):\n",
    "            stepwise_return = stepwise_return*self.discount_factor + r\n",
    "            stepwise_return_arr.append(stepwise_return)\n",
    "        tensor_norm = F.normalize(input=torch.tensor(list(reversed(stepwise_return_arr))),dim=0)\n",
    "        return tensor_norm.tolist() # > tensor([-0.1740, -0.1021, 0.3525,  0.4109,  0.4675,  0.5201])\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
