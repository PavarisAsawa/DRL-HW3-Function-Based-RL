{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 1: Understanding the Algorithm**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name : Pavaris Asawakijtananont\n",
    "\n",
    "Number : 65340500037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you have to implement 4 different function approximation-based RL algorithms:\n",
    "\n",
    "- **Linear Q-Learning**\n",
    "\n",
    "- **Deep Q-Network** (DQN)\n",
    "\n",
    "- **REINFORCE algorithm**\n",
    "\n",
    "- One algorithm chosen from the following Actor-Critic methods:\n",
    "    - **Deep Deterministic Policy Gradient** (DDPG)\n",
    "    - **Advantage Actor-Critic** (A2C)\n",
    "    - **Proximal Policy Optimization** (PPO)\n",
    "    - **Soft Actor-Critic** (SAC)\n",
    "\n",
    "For each algorithm, describe whether it follows a `value-based, policy-based, or Actor-Critic approach`, specify the `type of policy it learns` (stochastic or deterministic), identify the type of `observation space and action space` (discrete or continuous), and `explain how each advanced RL method balances exploration and exploitation`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref \n",
    "- https://github.com/johnnycode8/gym_solutions\n",
    "- https://arxiv.org/pdf/1312.5602\n",
    "- https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm\n",
    "- https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "- https://www.youtube.com/watch?v=EUrWGTCGzlA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Base Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Linear Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-Learning we have to update throght weight instead of direct to Q value from learning to select the action\n",
    "- and with linear function approximation we can use feature vector to directly update the action value or weight, this solution can handle with continuous value\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Updating Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the weight value to approximate Q-value by use linear approximation equation, with equation\n",
    "\n",
    "$$\n",
    "Q(s,a) = \\phi(s)^T w_a\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight can update by using gradient descent to converge to optimal action from state action pair, which we set the maximum action value along the state for all action to target policy add with reward value.\n",
    "\n",
    "$$\n",
    "w = w + \\alpha[r + \\gamma\\max_{a'} Q_\\pi(s',a') - Q(s,a)]X(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- target policy (same with normal Q Learning) : $r + \\gamma\\max_{a'} Q_\\pi(s',a')$\n",
    "- direction of gradient : using the observation term(equal to gradient of action value along the weight) to define direction\n",
    "    - Observation term $X(s) = \\nabla_{w} Q(s,a ; w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the equation we can coding in update function\n",
    "\n",
    "```python\n",
    "    def update(\n",
    "        self,\n",
    "        obs,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_obs,\n",
    "        next_action: int,\n",
    "        terminated: bool\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Updates the weight vector using the Temporal Difference (TD) error \n",
    "        in Q-learning with linear function approximation.\n",
    "\n",
    "        Args:\n",
    "            obs (dict): The current state observation, containing feature representations.\n",
    "            action (int): The action taken in the current state.\n",
    "            reward (float): The reward received for taking the action.\n",
    "            next_obs (dict): The next state observation.\n",
    "            next_action (int): The action taken in the next state (used in SARSA).\n",
    "            terminated (bool): Whether the episode has ended.\n",
    "\n",
    "        \"\"\"\n",
    "        # ========= put your code here ========= #\n",
    "        q_curr = self.q(obs=obs, action=action)\n",
    "        if terminated:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.discount_factor * np.max(self.q(next_obs))\n",
    "        pass\n",
    "    \n",
    "        error = target - q_curr\n",
    "        self.training_error.append(error)\n",
    "        # Gradient descent update\n",
    "        self.w[:, action] += self.lr * error * obs['policy'].detach().numpy()\n",
    "        # ====================================== #\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Action Selection**\n",
    "- Same as Q Learning , we use the epsilon greedy to select the action to explore or exploit with probability epsilon\n",
    "\n",
    "```python\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select an action based on an epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state (Tensor): The current state of the environment.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The selected action.\n",
    "        \"\"\"\n",
    "        # ========= put your code here ========= #\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Exploration: choose a random action\n",
    "            return np.random.randint(0, self.num_of_action)\n",
    "        else:\n",
    "            # Exploitation: choose the action with the highest estimated Q-value\n",
    "            return np.argmax(self.q(state))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question**\n",
    "- **Approach Type** : Value Based\n",
    "    - Directly estimated state-action value \n",
    "- **Policy Type** : Deterministic (with Stochastic Exploration)\n",
    "    - Policy choosing the action that maximizes the estimate Q-Value from policy\n",
    "    - In training process agent learn with epsilon-greedy(probability based)\n",
    "- **Observation and Action Spaces**\n",
    "    -   Observation Space: Linear Q-Learning is well-suited for environments with continuous observation spaces where `states are represented by feature vectors`.\n",
    "    -   Action Space: Discrete only.\n",
    "- **Balancing Exploration and Exploitation** : Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref \n",
    "- https://github.com/johnnycode8/gym_solutions\n",
    "- https://arxiv.org/pdf/1312.5602\n",
    "- https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm\n",
    "- https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "- https://www.youtube.com/watch?v=EUrWGTCGzlA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Q Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images\\Deep-Q-Learning-code.png\" alt=\"Description\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question**\n",
    "- **Approach Type** : Value Based\n",
    "- **Policy Type** : Deterministic\n",
    "    - Policy choosing the action that maximizes the estimate Q-Value from policy\n",
    "- **Observation and Action Spaces**\n",
    "    -   Observation Space: discrete or continuous\n",
    "    -   Action Space: Discrete only\n",
    "- **Balancing Exploration and Exploitation**\n",
    "    - Epsilon-greedy : decaying exploration rate over time\n",
    "    - Experience Replay : store experience buffer and sample mini-batches to update, this method break temporal correlation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref\n",
    "- https://thammasorn.github.io/2020/06/03/DQN.html\n",
    "- https://skrl.readthedocs.io/en/latest/api/agents/dqn.html\n",
    "- https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "- https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **REINFORCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref \n",
    "- https://thammasorn.github.io/2020/07/30/reinforce.html"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
