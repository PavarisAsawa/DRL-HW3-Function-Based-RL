{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 1: Understanding the Algorithm**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`Name`** : **Pavaris Asawakijtananont**\n",
    "\n",
    "- **`Number`** : **65340500037**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you have to implement 4 different function approximation-based RL algorithms:\n",
    "\n",
    "- **Linear Q-Learning**\n",
    "\n",
    "- **Deep Q-Network** (DQN)\n",
    "\n",
    "- **REINFORCE algorithm**\n",
    "\n",
    "- One algorithm chosen from the following Actor-Critic methods:\n",
    "    - **Deep Deterministic Policy Gradient** (DDPG)\n",
    "    - **Advantage Actor-Critic** (A2C)\n",
    "    - **Proximal Policy Optimization** (PPO)\n",
    "    - **Soft Actor-Critic** (SAC)\n",
    "\n",
    "For each algorithm, describe whether it follows a `value-based, policy-based, or Actor-Critic approach`, specify the `type of policy it learns` (stochastic or deterministic), identify the type of `observation space and action space` (discrete or continuous), and `explain how each advanced RL method balances exploration and exploitation`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref \n",
    "- https://github.com/johnnycode8/gym_solutions\n",
    "- https://arxiv.org/pdf/1312.5602\n",
    "- https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm\n",
    "- https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "- https://www.youtube.com/watch?v=EUrWGTCGzlA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Linear Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-Learning we have to update throght weight instead of direct to Q value from learning to select the action\n",
    "- and with linear function approximation we can use feature vector to directly update the action value or weight, this solution can handle with continuous value\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Updating Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the weight value to approximate Q-value by use linear approximation equation, with equation\n",
    "\n",
    "$$\n",
    "Q(s,a) = \\phi(s)^T w_a\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight can update by using gradient descent to converge to optimal action from state action pair, which we set the maximum action value along the state for all action to target policy add with reward value.\n",
    "\n",
    "$$\n",
    "w = w + \\alpha[r + \\gamma\\max_{a'} Q_\\pi(s',a') - Q(s,a)]X(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- target policy (same with normal Q Learning) : $r + \\gamma\\max_{a'} Q_\\pi(s',a')$\n",
    "- direction of gradient : using the observation term(equal to gradient of action value along the weight) to define direction\n",
    "    - Observation term $X(s) = \\nabla_{w} Q(s,a ; w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question**\n",
    "- **Approach Type** : Value Based\n",
    "    - Directly estimated state-action value \n",
    "- **Policy Type** : Deterministic (with Stochastic Exploration)\n",
    "    - Policy choosing the action that maximizes the estimate Q-Value from policy\n",
    "    - In training process agent learn with epsilon-greedy(probability based)\n",
    "- **Observation and Action Spaces**\n",
    "    -   Observation Space: Linear Q-Learning is well-suited for environments with continuous observation spaces where `states are represented by feature vectors`.\n",
    "    -   Action Space: Discrete only.\n",
    "- **Balancing Exploration and Exploitation** : Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref \n",
    "- https://github.com/johnnycode8/gym_solutions\n",
    "- https://arxiv.org/pdf/1312.5602\n",
    "- https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm\n",
    "- https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "- https://www.youtube.com/watch?v=EUrWGTCGzlA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Q Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Updating Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep Q network is use Neural Network as function approximator to approximate action value give the state, and same as the other Q Learning Algorithm we must have the target value to find loss function in this DQN we give target as:\n",
    "\n",
    "$$\n",
    "y = r_t + \\gamma \\max_a Q(s_{t+1} , a ; \\theta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images\\Deep-Q-Learning-code.png\" alt=\"Description\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And updating the Policy network by calculating with MSE loss\n",
    "\n",
    "$$\n",
    "L(\\theta)= (y_j - Q(\\phi_{j} , a_j ; \\theta))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and updating weight with\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\nabla (y_j +\\gamma \\max_{a'}Q(\\phi_{j+1} , a' ; \\theta))^2\n",
    "$$\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\nabla L(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DQN is difference from traditional Q Learning, DQN is collecting buffer from environment and update with batch size by updating with fixed time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images\\DQN_overview.png\" alt=\"Description\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images\\DQN_flow.png\" alt=\"Description\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to get action value DQN learning throught the weight in the neural network to find optimal weight\n",
    "- to get the action from policy DQN using deterministic policy (argument max) to find optimal value from action value\n",
    "- and get experience from environment, store into replay memory to updating both network(policy and target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Target Network Updating**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- updating from experience is sampling efficient method and can updating multiple time same experience or transition , to if only use one policy if we it like to step the target futher cause of correlation of the value (from same network)\n",
    "\n",
    "$$\n",
    " \\mathbb{E}_{(s,a,r,s')~U(D)}[((r + \\gamma \\max_a Q(s',a;\\theta) - Q(s,a;\\theta))^2)]\n",
    "$$\n",
    "\n",
    "- to solve this problem target network come out to, instead we using the same network we using the seperate network to act as target\n",
    "- The target network is must updating same as policy network\n",
    "    - to updating target we can set $\\theta^- = \\theta$ (updating weight of target network equal policy network) with fix timestep\n",
    "    - another method is soft updating with updating target network every timestep following the equation, when tau is small value to converge target network to policy\n",
    "        $$\n",
    "        \\theta^{-} = (\\tau)\\theta + (1-\\tau)\\theta^{-}\n",
    "        $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question**\n",
    "- **Approach Type** : Value Based\n",
    "- **Policy Type** : Deterministic\n",
    "    - Policy choosing the action that maximizes the estimate Q-Value from policy\n",
    "- **Observation and Action Spaces**\n",
    "    -   Observation Space: discrete or continuous\n",
    "    -   Action Space: Discrete only\n",
    "- **Balancing Exploration and Exploitation**\n",
    "    - Epsilon-greedy : decaying exploration rate over time\n",
    "    - Experience Replay : store experience buffer and sample mini-batches to update, this method break temporal correlation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref\n",
    "- https://thammasorn.github.io/2020/06/03/DQN.html\n",
    "- https://skrl.readthedocs.io/en/latest/api/agents/dqn.html\n",
    "- https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "- https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **REINFORCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- REINFORCE is policy based reinforcement learning and it is the policy gradient algorithm, base on policy\n",
    "- policy gradient is try to updating neural network parameter to estimate probability of action given the state to get maximize reward  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Stochastic Policy**\n",
    "- In policy gradient we using the stochastic policy to optimize the objective function\n",
    "    - this is different from deterministic policy that we will have probabilibty to get every action, this open the chance to more exploration with probability to select other action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images\\deterministic-vs-stochastic.png\" alt=\"Description\" width=\"750\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to implement stochastic policy in neural network we can applying `Softmax Function` to output with probability density function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gradient Ascent**\n",
    "- to calculate objective function of REINFORCE is using the reward directly to be a gradient but reward is scalar value that couldn't find the gradient\n",
    "$$\n",
    "\\mathbb{E} [\\sum_{t+1}^{\\infty}r_t] = \\frac{\\sum_{\\tau}\\sum_{t+1}^{\\infty}r_t^\\tau}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this equation mean expected return is mean of from reward from all transition state($\\tau$) everytime step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E} [\\sum_{t+1}^{\\infty}r_t] = \\sum_\\tau P(\\tau;\\theta)R(\\tau)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we must use some trick(log likelihood trick) to transfrom in to derivativable form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla \\sum_\\tau P(\\tau;\\theta)R(\\tau) =  \\sum_\\tau \\nabla P(\\tau;\\theta)R(\\tau) = \\sum_\\tau P(\\tau;\\theta) \\frac{\\nabla P(\\tau;\\theta)}{P(\\tau;\\theta)} R(\\tau)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we will get the final form of objective function to updating the network\n",
    "\n",
    "$$\n",
    "\\sum_\\tau P(\\tau;\\theta) \\nabla_{\\theta} \\log P(\\tau;\\theta) R(\\tau) = \\mathbb{E}_{\\tau \\sim \\pi_{\\tau}} \\nabla_{\\theta} \\log P(\\tau;\\theta) R(\\tau)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Updating Method**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- REINFORCE algorithm is same as Monte Carlo method to sampling the trajectory to collect experience and then rerun the trajectory to calculate return each time step and multiply with log probability following the equation and will get loss to backpropagate the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images\\reinforce-code.png\" alt=\"Description\" width=\"750\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question**\n",
    "- **Approach Type** : Policy-based \n",
    "- **Policy Type** : Stochastic Policy\n",
    "    - the policy outputs a probability distribution over actions\n",
    "- **Observation and Action Spaces**\n",
    "    -   Observation Space: discrete or continuous\n",
    "    -   Action Space: discrete or continuous\n",
    "- **Balancing Exploration and Exploitation**\n",
    "    - Stochastic Policy : normally random the action from probability distribution over action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref \n",
    "- https://thammasorn.github.io/2020/07/30/reinforce.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Actor Critic : Proximal Policy Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PPO is policy gradient method which alternate sanpling data from environment that interacton with and optimizing `surrogate` objective function with using stochastic gradient ascent\n",
    "- PPO have some benefit from TRPO but more general and simpler to implement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Policy Optimization**\n",
    "##### **Policy Gradient**\n",
    "- loss of policy gradient is given\n",
    "$$\n",
    "L^{PG} = \\hat{\\mathbb{E}}_t [\\log \\pi_\\theta (a_t | s_t) \\hat{A_t}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this loss is same as the REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Clipping Surrogate Objective**\n",
    "- TRPO is maximize a `surrogate` objective given\n",
    "\n",
    "$$\n",
    "r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)}\n",
    "$$\n",
    "$$\n",
    "L^{CLIP} = \\hat{\\mathbb{E}}_t [ \\min ( r_t(\\theta) \\hat{A_t}  , clip(r_t(\\theta) , 1-\\epsilon , 1+\\epsilon )\\hat{A_t} )]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images\\PPO-clipping.png\" alt=\"Description\" width=\"750\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- where epsilon is a hyperparameter, modifies the surrogateobjective by clipping the probability ratio, which removes the incentive for moving $r_t$ outside of the interval [1 − $\\epsilon$, 1 + $\\epsilon$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Final Loss**\n",
    "- contain with 3 term of loss\n",
    "\n",
    "$$\n",
    "L_t^{CLIP+VF+S}(\\theta) = \\hat{\\mathbb{E}}_t [ L_t^{CLIP}(\\theta) - c_1 L_t^{VF}(\\theta) + c_2 S[\\pi_{\\theta}](s_t) ]\n",
    "$$\n",
    "\n",
    "- c1 , c2 is constant value to multiplying with loss term\n",
    "- last term is `Entropy` bonus if calculate from entropy of output action is term can increase more exploration on agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Updating Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Advantage Calculation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In loss term we need to find `Advantages` to define how that action is good, **`GAE` [generalized advantage estimation]** is the choice in PPO that reduces the advantages full equation form\n",
    "\n",
    "$$\n",
    "\\hat{A}_t = -V(s_t)+r_t + \\gamma r_{t+1} + ... + \\gamma^{T-t+1}r_{T-1} + \\gamma^{T-t}V(s_T)\n",
    "$$\n",
    "\n",
    "to this form\n",
    "\n",
    "$$\n",
    "\\hat{A}_t = \\zeta_{t} + (\\gamma \\lambda)\\zeta_{t+1} +...+...+(\\gamma \\lambda)^{T-t+1} \\zeta_{T-1}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\zeta_t = r_t +\\gamma(s_{t+1})-V(s_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Collect Trajectory and Optimize surrogate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images\\PPO-code.png\" alt=\"Description\" width=\"750\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PPO is collecting sampling from **`Parallel actors`** and collect transition with fixed timestep, and then optimize surrogate loss on the `NT` timesteps ( Number of actor * timestep) with `K` epoch and optimize with batch size `M` when M $\\le$ NT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> More Detail more in PART 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question**\n",
    "- **Approach Type** : Policy-based / Actor - Critic\n",
    "- **Policy Type** : Stochastic Policy\n",
    "    - the policy outputs a probability distribution over actions\n",
    "- **Observation and Action Spaces**\n",
    "    -   Observation Space: discrete or continuous\n",
    "    -   Action Space: discrete or continuous\n",
    "- **Balancing Exploration and Exploitation**\n",
    "    - Stochastic Policy : normally random the action from probability distribution over action\n",
    "    - Clipping : Prevent excessive changes that might swing the policy too rapidly toward exploitation\n",
    "    - Entropy Bonus : This bonus encourages continuous exploration even as the policy improves.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref\n",
    "- https://arxiv.org/pdf/1707.06347\n",
    "- https://joel-baptista.github.io/phd-weekly-report/posts/ac/\n",
    "- https://github.com/saqib1707/RL-PPO-PyTorch/blob/main/src/ppo.py\n",
    "- https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py\n",
    "- https://www.youtube.com/watch?v=MEt6rrxH8W4\n",
    "- https://medium.com/@eyyu/coding-ppo-from-scratch-with-pytorch-part-2-4-f9d8b8aa938a ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration vs. Exploitation\n",
    "\n",
    "PPO trains a stochastic policy in an on-policy way. This means that it explores by sampling actions according to the latest version of its stochastic policy. The amount of randomness in action selection depends on both initial conditions and the training procedure. Over the course of training, the policy typically becomes progressively less random, as the update rule encourages it to exploit rewards that it has already found. This may cause the policy to get trapped in local optima"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
